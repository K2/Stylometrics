# Fusion Model Predictor GenAIScript Wrapper - API Notes

## Overview

This script (`fusion_model_predictor.genai.mts`) interfaces with the Python-based fusion model defined in `fusion_model.py` to predict whether a given text is AI-generated. It handles input text, executes the Python script via `host.exec`, and parses the prediction results (probabilities and binary predictions) from JSON output.

## Design Goals

*   **Integration:** Provide a GenAIScript interface for the prediction functionality of `fusion_model.py`.
*   **Input Handling:** Accept input text and manage temporary file creation.
*   **Execution:** Execute the Python prediction script.
*   **Output Parsing:** Parse the JSON output `[probabilities: number[], predictions: number[]]` from Python.
*   **Error Handling:** Report errors during execution.

## Key Components

*   **`script({...})`:** Defines the script and parameters (`inputText`, `modelPath`).
*   **`workspace.writeText()`:** Creates a temporary file for the input text.
*   **`host.exec()`:** Executes `python3 fusion_model.py <temp_file_path> [--model <model_path>]`.
*   **`JSON.parse()`:** Parses the `stdout`.

## Core Flow

1.  **Input:** Receives `inputText` and optional `modelPath`.
2.  **Temp File:** Writes `inputText` to a temporary file.
3.  **Execution:** Calls `host.exec` to run `python3 fusion_model.py` with the temp file path and optional model path argument.
4.  **Output Handling:** Parses the JSON array `[probabilities, predictions]` from `stdout` or reports errors.
5.  **Return:** Returns an object `{ probabilities: number[], predictions: number[] }` or an error structure.

## Constraints & Assumptions

*   **Python Environment:** Assumes `python3` is available in the host's PATH.
*   **Dependencies:** Assumes necessary Python libraries are installed (`numpy`, `torch`, potentially `transformers` or similar for the LM - as required by `fusion_model.py`).
*   **`fusion_model.py` Interface:** Assumes the script accepts a file path argument, an optional `--model` argument for the pre-trained model path, and prints a JSON array `[probabilities_array, predictions_array]` to `stdout`.
*   **Pre-trained Model:** Assumes a pre-trained fusion model exists and its path can be provided or is known by the Python script. This wrapper does *not* handle model training.

## Usage Example (Conceptual)

```genaiscript
const textToPredict = "This text might be generated by AI.";
const modelFile = "/path/to/trained_fusion_model.pth"; // Optional
const result = await runPrompt("fusion_model_predictor", {
    vars: { inputText: textToPredict, modelPath: modelFile }
});
if (!result.error) {
    console.log("Prediction Result:", result.json);
    // result.json should be like { probabilities: [0.85], predictions: [1] }
} else {
    console.error("Error:", result.error);
}
```

## Debugging

*   Check `stderr` from `host.exec` for Python errors (e.g., missing libraries like `torch`, model loading issues).
*   Verify the temporary input file.
*   Ensure `fusion_model.py` handles its command-line arguments (`--model`) correctly and outputs valid JSON `[array, array]`.
*   Confirm Python dependencies are installed (`pip install numpy torch transformers` or similar).
*   Ensure the specified model path is correct and accessible to the Python script.